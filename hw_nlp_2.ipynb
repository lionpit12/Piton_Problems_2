{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e1c3aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vmakh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d82f1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to preprocess the messages\n",
    "def preprocess_message(message):\n",
    "    # remove semicolons and quotes\n",
    "    message = re.sub('[;\"]+', '', message)\n",
    "\n",
    "    # convert to lowercase\n",
    "    message = message.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    message = message.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('russian')) \n",
    "    message_tokens = message.split()\n",
    "    filtered_tokens = [token for token in message_tokens if token not in stop_words]\n",
    "    message = ' '.join(filtered_tokens)\n",
    "\n",
    "    return message\n",
    "\n",
    "# read the csv file\n",
    "df = pd.read_csv('negative.csv', sep=';', header=None, usecols=[3, 4], names=['message', 'label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52fe5455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>на работе был полный пиддес :| и так каждое за...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Коллеги сидят рубятся в Urban terror, а я из-з...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@elina_4post как говорят обещаного три года жд...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Желаю хорошего полёта и удачной посадки,я буду...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Обновил за каким-то лешим surf, теперь не рабо...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label\n",
       "0  на работе был полный пиддес :| и так каждое за...     -1\n",
       "1  Коллеги сидят рубятся в Urban terror, а я из-з...     -1\n",
       "2  @elina_4post как говорят обещаного три года жд...     -1\n",
       "3  Желаю хорошего полёта и удачной посадки,я буду...     -1\n",
       "4  Обновил за каким-то лешим surf, теперь не рабо...     -1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddcb487a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>работе полный пиддес каждое закрытие месяца св...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>коллеги сидят рубятся urban terror изза долбан...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elina4post говорят обещаного года ждут</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>желаю хорошего полёта удачной посадкия буду оч...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>обновил какимто лешим surf работает простоплеер</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label\n",
       "0  работе полный пиддес каждое закрытие месяца св...     -1\n",
       "1  коллеги сидят рубятся urban terror изза долбан...     -1\n",
       "2             elina4post говорят обещаного года ждут     -1\n",
       "3  желаю хорошего полёта удачной посадкия буду оч...     -1\n",
       "4    обновил какимто лешим surf работает простоплеер     -1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the messages\n",
    "df['message'] = df['message'].apply(preprocess_message)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beaac359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1400/1400 [==============================] - 2016s 1s/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 1.4353e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1400/1400 [==============================] - 2415s 2s/step - loss: 4.5996e-06 - accuracy: 1.0000 - val_loss: 5.6573e-11 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1400/1400 [==============================] - 2910s 2s/step - loss: 7.0008e-06 - accuracy: 1.0000 - val_loss: 1.1480e-12 - val_accuracy: 1.0000\n",
      "700/700 [==============================] - 141s 202ms/step - loss: 1.1480e-12 - accuracy: 1.0000\n",
      "Test loss: 1.1480152765919183e-12\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# tokenize the messages\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['message'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert the messages to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['message'])\n",
    "\n",
    "# pad the sequences to have the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# convert the labels to categorical\n",
    "labels = to_categorical(df['label'], num_classes=2)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 128, input_length=100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=3, validation_data=(X_test, y_test))\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "# save the model\n",
    "model.save('my_nlp_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d8966",
   "metadata": {},
   "source": [
    "## **Задания**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2887c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "X=df['message']\n",
    "labels=df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8000843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91825   -1\n",
       "86999   -1\n",
       "86326   -1\n",
       "24572   -1\n",
       "58539   -1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4595df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "# выделение токенов с высокой частотой\n",
    "high_freq_vec = CountVectorizer(max_features=5000)\n",
    "X_train_high = high_freq_vec.fit_transform(X_train)\n",
    "\n",
    "# выделение токенов со средней частотой\n",
    "mid_freq_vec = CountVectorizer(max_features=5000, min_df=5)\n",
    "X_train_mid = mid_freq_vec.fit_transform(X_train)\n",
    "\n",
    "# выделение токенов с низкой частотой\n",
    "low_freq_vec = CountVectorizer(max_features=5000, min_df=2)\n",
    "X_train_low = low_freq_vec.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dd076e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High frequency tokens:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00     22385\n",
      "\n",
      "    accuracy                           1.00     22385\n",
      "   macro avg       1.00      1.00      1.00     22385\n",
      "weighted avg       1.00      1.00      1.00     22385\n",
      "\n",
      "Mid frequency tokens:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00     22385\n",
      "\n",
      "    accuracy                           1.00     22385\n",
      "   macro avg       1.00      1.00      1.00     22385\n",
      "weighted avg       1.00      1.00      1.00     22385\n",
      "\n",
      "Low frequency tokens:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00     22385\n",
      "\n",
      "    accuracy                           1.00     22385\n",
      "   macro avg       1.00      1.00      1.00     22385\n",
      "weighted avg       1.00      1.00      1.00     22385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# обучение классификаторов\n",
    "clf_high = MultinomialNB().fit(X_train_high, y_train)\n",
    "clf_mid = MultinomialNB().fit(X_train_mid, y_train)\n",
    "clf_low = MultinomialNB().fit(X_train_low, y_train)\n",
    "\n",
    "# получение предсказаний и вывод отчета классификации\n",
    "y_pred_high = clf_high.predict(high_freq_vec.transform(X_test))\n",
    "y_pred_mid = clf_mid.predict(mid_freq_vec.transform(X_test))\n",
    "y_pred_low = clf_low.predict(low_freq_vec.transform(X_test))\n",
    "\n",
    "print(\"High frequency tokens:\\n\", classification_report(y_test, y_pred_high))\n",
    "print(\"Mid frequency tokens:\\n\", classification_report(y_test, y_pred_mid))\n",
    "print(\"Low frequency tokens:\\n\", classification_report(y_test, y_pred_low))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6adaaf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее важные токены для классификатора, обученного на токенах с высокой частотой:\n",
      "['rt', 'это', 'хочу', 'сегодня', 'очень', 'день', 'завтра', 'могу', 'просто', 'блин']\n",
      "\n",
      "Наиболее важные токены для классификатора, обученного на токенах со средней частотой:\n",
      "['seconds', 'этими', 'хотят', 'сдохла', 'очередь', 'делают', 'забываю', 'можешь', 'просыпаться', 'блииин']\n",
      "\n",
      "Наиболее важные токены для классификатора, обученного на токенах с низкой частотой:\n",
      "['seconds', 'этих', 'хотят', 'сдохну', 'ощущение', 'делом', 'забыть', 'мокрая', 'просыпается', 'блииин']\n"
     ]
    }
   ],
   "source": [
    "# Задание 2\n",
    "\n",
    "# определение наиболее важных токенов для каждого класса\n",
    "tokens = high_freq_vec.get_feature_names_out()\n",
    "high_prob = np.exp(clf_high.feature_log_prob_)\n",
    "mid_prob = np.exp(clf_mid.feature_log_prob_)\n",
    "low_prob = np.exp(clf_low.feature_log_prob_)\n",
    "\n",
    "high_top_indices = np.argsort(high_prob[0])[::-1][:10]\n",
    "mid_top_indices = np.argsort(mid_prob[0])[::-1][:10]\n",
    "low_top_indices = np.argsort(low_prob[0])[::-1][:10]\n",
    "\n",
    "print(\"Наиболее важные токены для классификатора, обученного на токенах с высокой частотой:\")\n",
    "print([tokens[i] for i in high_top_indices])\n",
    "print()\n",
    "print(\"Наиболее важные токены для классификатора, обученного на токенах со средней частотой:\")\n",
    "print([tokens[i] for i in mid_top_indices])\n",
    "print()\n",
    "print(\"Наиболее важные токены для классификатора, обученного на токенах с низкой частотой:\")\n",
    "print([tokens[i] for i in low_top_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486cf42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
