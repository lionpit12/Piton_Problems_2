{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8812a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 40\n",
    "num_classes = 1\n",
    "\n",
    "# Training\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d3bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc6340e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a670d6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71ec8d",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d2a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vmakh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# функция для удаления ненужных символов\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # удаляем упоминания пользователей\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text) # удаляем ссылки\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # удаляем все, кроме букв и цифр\n",
    "    return text\n",
    "\n",
    "# функция для токенизации и удаления стоп-слов\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# применяем предобработку к обучающим данным\n",
    "df_train['tweet'] = df_train['tweet'].apply(preprocess_text)\n",
    "\n",
    "# применяем предобработку к тестовым данным\n",
    "df_test['tweet'] = df_test['tweet'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb850ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kids dysfun...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                          model love u take u time ur\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07bc53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = \" \".join(df_train[\"tweet\"])\n",
    "train_corpus = train_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a4fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vmakh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "tokens = word_tokenize(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1dc863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "from nltk.probability import FreqDist\n",
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b34a31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered_top[10:]\n",
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "273f4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return padding + result[-maxlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2917d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"tweet\"]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9221828e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 40)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a137a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,   95,   18,  331,  519,   21, 1374])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6007bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "seed = 0\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7c043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embedding_dim = 128, out_channel = 128, num_classes = 1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv = nn.Conv1d(embedding_dim, out_channel, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(out_channel, num_classes)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        output = self.embedding(x)\n",
    "        #                       B  F  L         \n",
    "        output = output.permute(0, 2, 1)\n",
    "        output = self.conv(output)\n",
    "        output = self.relu(output)\n",
    "        output = torch.max(output, axis=2).values\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f8b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target=None, transform=None):\n",
    "        self.data = torch.from_numpy(data).long()\n",
    "        if target is not None:\n",
    "            self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index] if self.target is not None else None\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e0429c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Parameters: 305409\n",
      "Train epoch 1/10\n",
      "Step 0: loss=0.9081712365150452\n",
      "Train epoch 2/10\n",
      "Step 0: loss=0.12539368867874146\n",
      "Train epoch 3/10\n",
      "Step 0: loss=0.07317684590816498\n",
      "Train epoch 4/10\n",
      "Step 0: loss=0.04047522693872452\n",
      "Train epoch 5/10\n",
      "Step 0: loss=0.0346423014998436\n",
      "Train epoch 6/10\n",
      "Step 0: loss=0.017977459356188774\n",
      "Train epoch 7/10\n",
      "Step 0: loss=0.009297455660998821\n",
      "Train epoch 8/10\n",
      "Step 0: loss=0.01334747951477766\n",
      "Train epoch 9/10\n",
      "Step 0: loss=0.02001524530351162\n",
      "Train epoch 10/10\n",
      "Step 0: loss=0.017678484320640564\n"
     ]
    }
   ],
   "source": [
    "model = Net(vocab_size=max_words)\n",
    "\n",
    "print(model)\n",
    "print(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "model.train()\n",
    "#model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=10e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    \n",
    "train_dataset = DataWrapper(x_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#test_dataset = DataWrapper(x_test, df_test['label'].values)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Train epoch {epoch}/{epochs}\")\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # data = data.cuda()\n",
    "        # target = target.cuda()\n",
    "        \n",
    "        # compute output\n",
    "        output = model(data)\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        loss = criterion(output, target.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%print_batch_n == 0:\n",
    "            loss = loss.float().item()\n",
    "            print(\"Step {}: loss={}\".format(i, loss))\n",
    "            loss_history.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13ec0579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss history')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Train loss')\n",
    "plt.xlabel('Step')\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18c3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DataWrapper(x_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1bfe151",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataWrapper' object has no attribute 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mDataWrapper.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index]\n\u001b[1;32m---> 12\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget[index] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     15\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(x)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataWrapper' object has no attribute 'target'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "   \n",
    "    for inputs, labels in test_loader:\n",
    "       \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "       \n",
    "        loss = loss_function(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    \n",
    "    print('Accuracy on test set: {:.2f}%'.format(accuracy))\n",
    "    print('Average loss on test set: {:.4f}'.format(average_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загрузка предобработанных данных\n",
    "df_train = pd.read_csv('train_preprocessed.csv')\n",
    "\n",
    "# Создание матрицы признаков и вектора целевой переменной\n",
    "X = df_train['tweet'].values\n",
    "y = df_train['label'].values\n",
    "\n",
    "# Разделение данных на обучающую и валидационную выборки\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Преобразование текстовых данных в числовые\n",
    "vocab_size = 2000\n",
    "max_len = 40\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "train_X = pad_sequences(train_X, maxlen=max_len)\n",
    "val_X = pad_sequences(val_X, maxlen=max_len)\n",
    "\n",
    "# Создание DataLoader\n",
    "train_data = TensorDataset(torch.tensor(train_X), torch.tensor(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940ed2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kids dysfun...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                          model love u take u time ur\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510b29af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Using cached torchtext-0.15.1-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Collecting torchdata==0.6.0\n",
      "  Using cached torchdata-0.6.0-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torchtext) (4.64.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torchtext) (1.21.5)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torchtext) (2.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (4.4.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from torchdata==0.6.0->torchtext) (1.26.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from requests->torchtext) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0->torchtext) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vmakh\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
      "Installing collected packages: torchdata, torchtext\n",
      "Successfully installed torchdata-0.6.0 torchtext-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77359f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vmakh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# создаем наборы данных и загрузчики\u001b[39;00m\n\u001b[0;32m     59\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TwitterDataset(df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m], df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer, max_len)\n\u001b[1;32m---> 60\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m TwitterDataset(df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, tokenizer, max_len)\n\u001b[0;32m     61\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     62\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Загружаем стоп-слова\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# задаем параметры токенизации\n",
    "max_words = 2000\n",
    "max_len = 40\n",
    "\n",
    "# создаем токенизатор и обучаем его на обучающих данных\n",
    "def tokenizer(text):\n",
    "    # токенизируем текст\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # удаляем стоп-слова\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "word_counter = {}\n",
    "for tweet in df_train['tweet']:\n",
    "    for word in tokenizer(tweet):\n",
    "        if word not in word_counter:\n",
    "            word_counter[word] = 1\n",
    "        else:\n",
    "            word_counter[word] += 1\n",
    "\n",
    "tokenizer = {word: i for i, word in enumerate(sorted(word_counter, key=word_counter.get, reverse=True)[:max_words])}\n",
    "\n",
    "# создаем класс для набора данных\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_tweet = tokenizer(tweet)\n",
    "        tokenized_tweet = tokenized_tweet[:self.max_len]\n",
    "        padded_tweet = torch.zeros(self.max_len, dtype=torch.long)\n",
    "        for i, token in enumerate(tokenized_tweet):\n",
    "            if token in self.tokenizer:\n",
    "                padded_tweet[i] = self.tokenizer[token]\n",
    "        return padded_tweet, label\n",
    "\n",
    "# создаем наборы данных и загрузчики\n",
    "train_dataset = TwitterDataset(df_train['tweet'], df_train['label'], tokenizer, max_len)\n",
    "test_dataset = TwitterDataset(df_test['tweet'], df_test['label'], tokenizer, max_len)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# создаем модель нейронной сети\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# задаем параметры модели\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "num_classes = 1\n",
    "\n",
    "# создаем экземпляр модели\n",
    "model = SentimentClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7d8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
