{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84aa66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0eb90b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Марш!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Иди.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Идите.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Здравствуйте.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Привет!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0              1                                                  2\n",
       "0  Go.          Марш!  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1  Go.           Иди.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2  Go.         Идите.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3  Hi.  Здравствуйте.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "4  Hi.        Привет!  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"rus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    df = pd.DataFrame([row for row in reader], columns=[n for n in range(3)])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6235ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Марш!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Иди.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Идите.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Здравствуйте.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Привет!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0              1\n",
       "0  Go.          Марш!\n",
       "1  Go.           Иди.\n",
       "2  Go.         Идите.\n",
       "3  Hi.  Здравствуйте.\n",
       "4  Hi.        Привет!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(2, axis=1)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e28bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392305, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a2c1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392300</th>\n",
       "      <td>In today's world, we have to equip all our kid...</td>\n",
       "      <td>В современном мире перед нами стоит задача дат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392301</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>Смерть - это зачастую то, разговоры или даже м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392302</th>\n",
       "      <td>At a moment when our economy is growing, our b...</td>\n",
       "      <td>В тот момент, когда наша экономика растёт, наш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392303</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Поскольку сайтов, посвящённых какой-либо теме,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392304</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>Несомненно, для каждого мужчины в этом мире гд...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "392300  In today's world, we have to equip all our kid...   \n",
       "392301  Death is something that we're often discourage...   \n",
       "392302  At a moment when our economy is growing, our b...   \n",
       "392303  Since there are usually multiple websites on a...   \n",
       "392304  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                    input  \n",
       "392300  В современном мире перед нами стоит задача дат...  \n",
       "392301  Смерть - это зачастую то, разговоры или даже м...  \n",
       "392302  В тот момент, когда наша экономика растёт, наш...  \n",
       "392303  Поскольку сайтов, посвящённых какой-либо теме,...  \n",
       "392304  Несомненно, для каждого мужчины в этом мире гд...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={0: \"output\", 1: \"input\"})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bf6ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0512a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df):\n",
    "    word_pairs = [[preprocess_sentence(w) for w in df[['input', 'output']].values[i]] for i in range(len(df))]\n",
    "    return zip(*word_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "en, ru = create_dataset(df)\n",
    "print(en[10])\n",
    "print(ru[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05346ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmakh\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"  # replace with the desired model name\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# prepare the data for fine-tuning\n",
    "#train_data = []\n",
    "#for i, row in df.iterrows():\n",
    "    #train_data.append({\n",
    "        #'input_text': row['rus'],\n",
    "        #'output_text': row['eng']\n",
    "    #})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6b5285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392305 392305\n"
     ]
    }
   ],
   "source": [
    "inputs=df['input'].tolist()\n",
    "outputs=df['output'].tolist()\n",
    "print(len(inputs), len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44061d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_encoded = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs_encoded = tokenizer(outputs, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4489dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 6063,  1492,    56,  ..., 62517, 62517, 62517],\n",
      "        [  137,   955,     3,  ..., 62517, 62517, 62517],\n",
      "        [  137,  4069,    30,  ..., 62517, 62517, 62517],\n",
      "        ...,\n",
      "        [   49,  1066,  2114,  ..., 62517, 62517, 62517],\n",
      "        [ 3987,    21, 32832,  ..., 62517, 62517, 62517],\n",
      "        [26173,     2,    40,  ...,    60,     3,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3af03bd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "training mode is expected to be boolean",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_encoded\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:2285\u001b[0m, in \u001b[0;36mModule.train\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the module in training mode.\u001b[39;00m\n\u001b[0;32m   2271\u001b[0m \n\u001b[0;32m   2272\u001b[0m \u001b[38;5;124;03mThis has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2282\u001b[0m \u001b[38;5;124;03m    Module: self\u001b[39;00m\n\u001b[0;32m   2283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m-> 2285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m   2287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n",
      "\u001b[1;31mValueError\u001b[0m: training mode is expected to be boolean"
     ]
    }
   ],
   "source": [
    "model.train(inputs_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance\n",
    "test_data = [\n",
    "    {'input_text': 'привет', 'target_text': 'hello'},\n",
    "    {'input_text': 'книга', 'target_text': 'book'},\n",
    "    # add more test data\n",
    "]\n",
    "test_df = pd.DataFrame(test_data)\n",
    "inputs = tokenizer(test_df['input_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = tokenizer(test_df['target_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "predictions = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "accuracy = (test_df['target_text'] == decoded_preds).mean()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8140fafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.', 'Go.', 'Go.', 'Hi.', 'Hi.', 'Hi.', 'Hi.', 'Hi.', 'Run!', 'Run!']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['eng'].values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc799083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec2db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d2f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = [(rus, eng) for rus, eng in train_df[[\"rus\", \"eng\"]].values]\n",
    "test_seq = [(rus, eng) for rus, eng in test_df[[\"rus\", \"eng\"]].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517ec672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Том не мог надеть свои носки.', \"Tom couldn't put his socks on.\"),\n",
       " ('Я хотел бы с тобой как-нибудь спеть.',\n",
       "  \"I'd like to sing with you sometime.\"),\n",
       " ('Я поехал туда на поезде.', 'I went there by train.'),\n",
       " ('До летних каникул всего неделя.',\n",
       "  'The summer vacation is only a week away.'),\n",
       " ('Этому дереву около трёхсот лет.', 'This tree is about 300 years old.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be751f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dc48869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmakh\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\n",
    "\n",
    "# Prepare the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a355c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "train_encoded = tokenizer(train_seq, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "test_encoded = tokenizer(test_seq, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6da3656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c0abaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1118,    46,    26,  1732,   730,    30,   320,   570,   217,  4215,\n",
       "            3,  1089,  5717,    21,  3590,  3672,   136,   189,    18,    45,\n",
       "          562,  6825,  3861,  2674,   144,   573, 11100,    23,    25,     3,\n",
       "            0, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517,\n",
       "        62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39361146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313844\n",
      "78461\n"
     ]
    }
   ],
   "source": [
    "print(len(train_encoded[\"input_ids\"]))\n",
    "print(len(test_encoded[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf05a531",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 27\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Define the Seq2SeqTrainer and train the model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m                                                )\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1909\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1906\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1908\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1909\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1910\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer_utils.py:704\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[0;32m    703\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:109\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[1;32m--> 109\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mvars\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m    110\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[1;32m--> 109\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m    110\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "# Define the training arguments for the Seq2SeqTrainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "# Define the Seq2SeqTrainer and train the model\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=torch.utils.data.TensorDataset(train_encodings['input_ids'],\n",
    "                                                 train_encodings['attention_mask']\n",
    "                                                 ),\n",
    "    eval_dataset=torch.utils.data.TensorDataset(test_encodings['input_ids'],\n",
    "                                                test_encodings['attention_mask']\n",
    "                                               )\n",
    ")\n",
    "                                               \n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafc687d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b860ce8bed494192aee726d44e8a29dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/307M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmakh\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vmakh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34238f13eff34978a89d587b769779dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc10c251cb04e11aeebb5545b367d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/target.spm:   0%|          | 0.00/803k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0d7334a927416fab981f14c2eb0bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmakh\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TranslationPipeline' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m translator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation_ru_to_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-ru-en\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train the pipeline on the df, limiting the output translation to 40 characters\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrus\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the pipeline\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(translator)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TranslationPipeline' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for translation Russian to English\n",
    "\n",
    "translator = pipeline(\"translation_ru_to_en\", model=\"Helsinki-NLP/opus-mt-ru-en\")\n",
    "\n",
    "\n",
    "\n",
    "# Train the pipeline on the df, limiting the output translation to 40 characters\n",
    "translator.fit(df[\"rus\"], df[\"eng\"], max_length=40)\n",
    "\n",
    "# Print the pipeline\n",
    "print(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ded8960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Hey, how's it going?\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for translation Russian to English\n",
    "translator = pipeline(\"translation_ru_to_en\", model=\"Helsinki-NLP/opus-mt-ru-en\")\n",
    "\n",
    "# Translate some text\n",
    "translated_text = translator(\"Привет, как дела?\")\n",
    "\n",
    "# Print the translated text\n",
    "print(translated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e7e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
